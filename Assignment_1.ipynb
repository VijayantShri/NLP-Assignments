{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG1To4R76Lv_",
        "colab_type": "text"
      },
      "source": [
        "#**Assignment-1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA_lMoD0T0QR",
        "colab_type": "text"
      },
      "source": [
        "For this task `nltk` is to be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAGikQzE2UEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk   # importing necessary library"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5DODsYB6dVc",
        "colab_type": "text"
      },
      "source": [
        "Download the basic package `popular` of `nltk` module to start with the NLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAyxStIV2cOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "outputId": "4a51fbdc-5c6a-4465-81dc-6782ae2ccbf3"
      },
      "source": [
        "nltk.download(\"popular\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8IuK7-c7PIi",
        "colab_type": "text"
      },
      "source": [
        "##**Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWDmmJfU512",
        "colab_type": "text"
      },
      "source": [
        "###**1. Word Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjzdbBy17aRg",
        "colab_type": "text"
      },
      "source": [
        "By using `word_tokenize` from `nltk.tokenize` word tokenization is done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncEp_MVF3u-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize as wt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVMXacWI43qG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39fc0799-3fa6-43bc-b93a-5b0931580c2d"
      },
      "source": [
        "text = \"Assignment is submitted by Vijayant Shrivastav of IIIT Bhopal with CSE branch in VII semester.\"\n",
        "print(wt(text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Assignment', 'is', 'submitted', 'by', 'Vijayant', 'Shrivastav', 'of', 'IIIT', 'Bhopal', 'with', 'CSE', 'branch', 'in', 'VII', 'semester', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXAVKXmX5vp_",
        "colab_type": "text"
      },
      "source": [
        "### **2. Sentence Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv2hV4Eq75H1",
        "colab_type": "text"
      },
      "source": [
        "Use `sent_tokenize` method of `nltk.tokenize` to tokenize sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEALZONi5kxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize as st"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSfjBRUw8TMy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7004d87-6fa5-4aaa-ca2a-a96f9787093b"
      },
      "source": [
        "print(st(text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Assignment is submitted by Vijayant Shrivastav of IIIT Bhopal with CSE branch in VII semester.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV9Jhh748lF3",
        "colab_type": "text"
      },
      "source": [
        "##**Stop Words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rag5DUpv8_RE",
        "colab_type": "text"
      },
      "source": [
        "Stop words are words that are filtered out before or after the natural language data (text) are processed. While “stop words” typically refers to the most common words in a language, all-natural language processing tools don't use a single universal list of stop words.\n",
        "\n",
        "Use predefined `stopwords` in `nltk.corpus` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kn027Iu8ehZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords as sw"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0KFczzB9fqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a sample paragraph.\n",
        "text_paragraph = \"The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.[4] NLTK includes graphical demonstrations and sample data.\"\n",
        "\n",
        "# Now store all the stopwords of english language in one set.\n",
        "stop_words = set(sw.words('english'))\n",
        "\n",
        "# Tokenize the words in the paragraph.\n",
        "word_tokens = wt(text_paragraph)\n",
        "\n",
        "# Creating a list to store the filtered words other than stopwords\n",
        "filter_words = []\n",
        "# Filtering the tokenized words\n",
        "for word in word_tokens:\n",
        "  if word not in stop_words:\n",
        "    filter_words.append(word)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBby4lDf_609",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "007cb5c8-2e46-4f12-ab22-5ba99dda1be4"
      },
      "source": [
        "# Let's print the list of tokenized words. \n",
        "print(word_tokens)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Natural', 'Language', 'Toolkit', ',', 'or', 'more', 'commonly', 'NLTK', ',', 'is', 'a', 'suite', 'of', 'libraries', 'and', 'programs', 'for', 'symbolic', 'and', 'statistical', 'natural', 'language', 'processing', '(', 'NLP', ')', 'for', 'English', 'written', 'in', 'the', 'Python', 'programming', 'language', '.', 'It', 'was', 'developed', 'by', 'Steven', 'Bird', 'and', 'Edward', 'Loper', 'in', 'the', 'Department', 'of', 'Computer', 'and', 'Information', 'Science', 'at', 'the', 'University', 'of', 'Pennsylvania', '.', '[', '4', ']', 'NLTK', 'includes', 'graphical', 'demonstrations', 'and', 'sample', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjwEIvBpAPnj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "af887541-56b0-43f7-915f-ba36536a5047"
      },
      "source": [
        "# Let's print the list of filtered words. \n",
        "print(filter_words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Natural', 'Language', 'Toolkit', ',', 'commonly', 'NLTK', ',', 'suite', 'libraries', 'programs', 'symbolic', 'statistical', 'natural', 'language', 'processing', '(', 'NLP', ')', 'English', 'written', 'Python', 'programming', 'language', '.', 'It', 'developed', 'Steven', 'Bird', 'Edward', 'Loper', 'Department', 'Computer', 'Information', 'Science', 'University', 'Pennsylvania', '.', '[', '4', ']', 'NLTK', 'includes', 'graphical', 'demonstrations', 'sample', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqrkFtI5AqUz",
        "colab_type": "text"
      },
      "source": [
        "##**Stemming**\n",
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.\n",
        "\n",
        "Use `PorterStemmer` of `porter` class to perform this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuDgZR58AWyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "ea50403a-062b-4dba-c5a5-011257da3e00"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Create the object of PorterStemmer class\n",
        "port_stem = PorterStemmer()\n",
        "\n",
        "# Use the filtered words and print the final steeming result.\n",
        "for word in filter_words:\n",
        "  print((word, port_stem.stem(word)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('The', 'the')\n",
            "('Natural', 'natur')\n",
            "('Language', 'languag')\n",
            "('Toolkit', 'toolkit')\n",
            "(',', ',')\n",
            "('commonly', 'commonli')\n",
            "('NLTK', 'nltk')\n",
            "(',', ',')\n",
            "('suite', 'suit')\n",
            "('libraries', 'librari')\n",
            "('programs', 'program')\n",
            "('symbolic', 'symbol')\n",
            "('statistical', 'statist')\n",
            "('natural', 'natur')\n",
            "('language', 'languag')\n",
            "('processing', 'process')\n",
            "('(', '(')\n",
            "('NLP', 'nlp')\n",
            "(')', ')')\n",
            "('English', 'english')\n",
            "('written', 'written')\n",
            "('Python', 'python')\n",
            "('programming', 'program')\n",
            "('language', 'languag')\n",
            "('.', '.')\n",
            "('It', 'It')\n",
            "('developed', 'develop')\n",
            "('Steven', 'steven')\n",
            "('Bird', 'bird')\n",
            "('Edward', 'edward')\n",
            "('Loper', 'loper')\n",
            "('Department', 'depart')\n",
            "('Computer', 'comput')\n",
            "('Information', 'inform')\n",
            "('Science', 'scienc')\n",
            "('University', 'univers')\n",
            "('Pennsylvania', 'pennsylvania')\n",
            "('.', '.')\n",
            "('[', '[')\n",
            "('4', '4')\n",
            "(']', ']')\n",
            "('NLTK', 'nltk')\n",
            "('includes', 'includ')\n",
            "('graphical', 'graphic')\n",
            "('demonstrations', 'demonstr')\n",
            "('sample', 'sampl')\n",
            "('data', 'data')\n",
            "('.', '.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDcC-2ysP4UF",
        "colab_type": "text"
      },
      "source": [
        "##**Lemmatization**\n",
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
        "\n",
        "Use `WordNetLemmatizer` of `nltk.stem` to perform lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyibLCXuPsj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "8228a278-8977-41c8-987b-5bd0774db6ac"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create the object of WordNetLammatizer class.\n",
        "wordNetLemm = WordNetLemmatizer()\n",
        "\n",
        "# Now lemmatize the filtered words.\n",
        "for word in filter_words:\n",
        "  print((word, wordNetLemm.lemmatize(word)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('The', 'The')\n",
            "('Natural', 'Natural')\n",
            "('Language', 'Language')\n",
            "('Toolkit', 'Toolkit')\n",
            "(',', ',')\n",
            "('commonly', 'commonly')\n",
            "('NLTK', 'NLTK')\n",
            "(',', ',')\n",
            "('suite', 'suite')\n",
            "('libraries', 'library')\n",
            "('programs', 'program')\n",
            "('symbolic', 'symbolic')\n",
            "('statistical', 'statistical')\n",
            "('natural', 'natural')\n",
            "('language', 'language')\n",
            "('processing', 'processing')\n",
            "('(', '(')\n",
            "('NLP', 'NLP')\n",
            "(')', ')')\n",
            "('English', 'English')\n",
            "('written', 'written')\n",
            "('Python', 'Python')\n",
            "('programming', 'programming')\n",
            "('language', 'language')\n",
            "('.', '.')\n",
            "('It', 'It')\n",
            "('developed', 'developed')\n",
            "('Steven', 'Steven')\n",
            "('Bird', 'Bird')\n",
            "('Edward', 'Edward')\n",
            "('Loper', 'Loper')\n",
            "('Department', 'Department')\n",
            "('Computer', 'Computer')\n",
            "('Information', 'Information')\n",
            "('Science', 'Science')\n",
            "('University', 'University')\n",
            "('Pennsylvania', 'Pennsylvania')\n",
            "('.', '.')\n",
            "('[', '[')\n",
            "('4', '4')\n",
            "(']', ']')\n",
            "('NLTK', 'NLTK')\n",
            "('includes', 'includes')\n",
            "('graphical', 'graphical')\n",
            "('demonstrations', 'demonstration')\n",
            "('sample', 'sample')\n",
            "('data', 'data')\n",
            "('.', '.')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}